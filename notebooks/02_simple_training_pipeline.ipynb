{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6182fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import os, glob\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.data import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dcc16bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConstructorError",
     "evalue": "could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:numpy.core.multiarray.scalar'\n  in \"../data.yaml\", line 6, column 13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-36ff95879b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# The FullLoader parameter handles the conversion from YAML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# scalar values to Python the dictionary format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mget_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_generators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_generators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdummy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_yaml_map\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_mapping\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMappingNode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_yaml_null\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_mapping\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 raise ConstructorError(\"while constructing a mapping\", node.start_mark,\n\u001b[1;32m    142\u001b[0m                         \"found unhashable key\", key_node.start_mark)\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_object\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mconstructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag_suffix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/yaml/constructor.py\u001b[0m in \u001b[0;36mconstruct_undefined\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_undefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         raise ConstructorError(None, None,\n\u001b[0m\u001b[1;32m    428\u001b[0m                 \u001b[0;34m\"could not determine a constructor for the tag %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 node.start_mark)\n",
      "\u001b[0;31mConstructorError\u001b[0m: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:numpy.core.multiarray.scalar'\n  in \"../data.yaml\", line 6, column 13"
     ]
    }
   ],
   "source": [
    "with open('../data.yaml', 'r') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    data = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f53acd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4#data[\"num_classes\"]\n",
    "RESUME = False\n",
    "epochs = 10\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "WEIGHTS_DIR = \"../weights\"\n",
    "\n",
    "Path(WEIGHTS_DIR).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88e1c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, Lambda\n",
    "from torchvision.transforms import ColorJitter, RandomAffine, RandomPerspective, RandomRotation, RandomErasing, RandomCrop, Grayscale\n",
    "from torchvision.transforms import RandomChoice, RandomApply\n",
    "\n",
    "def get_train_grayscale_transforms(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        RandomApply([\n",
    "            ColorJitter(brightness=0.3, contrast=0.01, saturation=0.01, hue=0),\n",
    "            RandomAffine(0.1, translate=(0.04,0.04), scale=(0.04,0.04), shear=0.01, resample=2),\n",
    "            RandomCrop(30),\n",
    "            RandomPerspective(0.1)\n",
    "        ]),\n",
    "        Resize([img_size, img_size], interpolation=3),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067])\n",
    "    ])\n",
    "def get_test_grayscale_transforms(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        Resize([img_size, img_size], interpolation=3),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e284819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_imgs = glob.glob(\"../../../Datasets/train_test_classification_full_size/train/*/*.jpg\")\n",
    "valid_imgs = glob.glob(\"../../../Datasets/train_test_classification_full_size/valid/*/*.jpg\")\n",
    "\n",
    "\n",
    "train_labels = set([os.path.basename(os.path.dirname(img_path)) for img_path in train_imgs])\n",
    "valid_labels = set([os.path.basename(os.path.dirname(img_path)) for img_path in train_imgs])\n",
    "class_to_idx = {label: idx for idx, label in enumerate(train_labels)}\n",
    "train_dataset = CustomImageDataset(train_imgs, get_test_grayscale_transforms(IMG_SIZE), train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_dataset = CustomImageDataset(train_imgs, get_test_grayscale_transforms(IMG_SIZE), valid_labels)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50fc26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080\n",
      "['indeterminate', 'typical', 'atypical', 'negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/10 Train: 0.7898: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "Test: 6.4442: 100%|██████████| 129/129 [02:32<00:00,  1.19s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/10 Train: 0.6426: 100%|██████████| 129/129 [02:36<00:00,  1.21s/it]\n",
      "Test: 7.2397: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/10 Train: 0.953: 100%|██████████| 129/129 [02:38<00:00,  1.23s/it] \n",
      "Test: 6.6636: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/10 Train: 0.878: 100%|██████████| 129/129 [02:38<00:00,  1.23s/it] \n",
      "Test: 5.9539: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/10 Train: 0.7604: 100%|██████████| 129/129 [02:39<00:00,  1.23s/it]\n",
      "Test: 6.6615: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/10 Train: 0.7965: 100%|██████████| 129/129 [02:38<00:00,  1.23s/it]\n",
      "Test: 4.7412: 100%|██████████| 129/129 [02:33<00:00,  1.19s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/10 Train: 0.7324: 100%|██████████| 129/129 [02:35<00:00,  1.21s/it]\n",
      "Test: 4.665: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it] \n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/10 Train: 1.0051: 100%|██████████| 129/129 [02:39<00:00,  1.23s/it]\n",
      "Test: 6.3378: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/10 Train: 0.7246: 100%|██████████| 129/129 [02:39<00:00,  1.23s/it]\n",
      "Test: 5.0506: 100%|██████████| 129/129 [02:34<00:00,  1.20s/it]\n",
      "  0%|          | 0/129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/10 Train: 0.6824: 100%|██████████| 129/129 [02:39<00:00,  1.23s/it]\n",
      "Test: 4.554: 100%|██████████| 129/129 [02:35<00:00,  1.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.319, F1: 0.484, Recall: 1.0, Precision: 0.319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'display_missclassified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-281a3ddf0558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m }, os.path.join(WEIGHTS_DIR, \"last.pt\"))\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mdisplay_missclassified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0mshow_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display_missclassified' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from utils.models import get_resnet18, get_efficientnetb0\n",
    "\n",
    "# Using gpu or not\n",
    "CUDA = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if CUDA == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "model = get_efficientnetb0(NUM_CLASSES, num_channels=1)\n",
    "#model = get_resnet18(NUM_CLASSES)\n",
    "#model = get_ghostnet(NUM_CLASSES)\n",
    "model.to(CUDA)\n",
    "print(list(class_to_idx.keys()))\n",
    "\n",
    "if RESUME:\n",
    "    optimizer, criterion = get_training_stuff(model)\n",
    "else:\n",
    "    #weights = torch.Tensor(weights).to(CUDA)\n",
    "    #optimizer, criterion = get_training_stuff(model, weights=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "if RESUME:\n",
    "    start_epoch = state_dict[\"epoch\"]\n",
    "    optimizer_state_dict = state_dict[\"optimizer_state_dict\"]\n",
    "    best_test_f1 = state_dict[\"best_test_f1\"]\n",
    "\n",
    "    model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state_dict[\"optimizer_state_dict\"])\n",
    "else:\n",
    "    best_test_f1 = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    t = tqdm(train_dataloader)\n",
    "    for i, (X, y) in enumerate(t):\n",
    "\n",
    "        X = X.to(CUDA)\n",
    "        y = y.to(CUDA)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.cpu().detach()\n",
    "        t.set_description(f\"{epoch+1}/{epochs} Train: {round(float(running_loss)/(i+1), 4)}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = list() # For display purpose\n",
    "        targets = list() # For display purpose\n",
    "        if epoch+1 == epochs:\n",
    "            images = list() # For display purpose\n",
    "        running_loss = 0\n",
    "        t = tqdm(valid_dataloader)\n",
    "        for i, (X, y) in enumerate(t):\n",
    "            X = X.to(CUDA)\n",
    "            y = y.to(CUDA)\n",
    "\n",
    "            preds = model(X)\n",
    "            predictions += list(preds.argmax(axis=1).cpu().detach().numpy())\n",
    "            targets += list(np.array(y.cpu()))\n",
    "            if epoch+1 == epochs:\n",
    "                images += list(np.array(X.cpu()))\n",
    "\n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            running_loss += loss.cpu().detach()\n",
    "            t.set_description(f\"Test: {round(float(running_loss/(i+1)), 4)}\")\n",
    "\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "    recall = recall_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "    precision = precision_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "    print(f\"Test: Acc: {str(acc)[:5]}, F1: {str(f1)[:5]}, Recall: {str(recall)[:5]}, Precision: {str(precision)[:5]}\\n\")\n",
    "\n",
    "    if f1 > best_test_f1:\n",
    "        best_test_f1 = f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_test_f1': best_test_f1,\n",
    "            'train_dataloader': train_dataloader,\n",
    "            'test_dataloader': valid_dataloader,\n",
    "            'class_to_idx': class_to_idx\n",
    "        }, os.path.join(WEIGHTS_DIR, \"best.pt\"))\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_test_f1': best_test_f1,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'test_dataloader': valid_dataloader,\n",
    "    'class_to_idx': class_to_idx,\n",
    "}, os.path.join(WEIGHTS_DIR, \"last.pt\"))\n",
    "\n",
    "display_missclassified(class_to_idx, targets, predictions, images, gridsize=(4,4))\n",
    "show_confusion_matrix(confusion_matrix(targets, predictions), list(class_to_idx.keys()))\n",
    "\n",
    "def show_confusion_matrix(matrix: List[List], labels: List[str]):\n",
    "    \"\"\"Display a nice confusion matrix given\n",
    "    the confusion matrix in a 2D list + list of labels (decoder)\n",
    "    \n",
    "    Args:\n",
    "        matrix: 2D array containing the values to display (confusion matrix)\n",
    "        labels: Array containing the labels (indexed by corresponding label idx)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    min_val, max_val = 0, len(labels)\n",
    "\n",
    "    for i in range(max_val):\n",
    "        for j in range(max_val):\n",
    "            c = matrix[i][j]\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center')\n",
    "\n",
    "    ax.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_xticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_xticklabels(labels, rotation='vertical', fontsize=16)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_yticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_yticklabels(labels, rotation='horizontal', fontsize=16)\n",
    "                    \n",
    "    #ax.set_xlim(min_val, max_val)\n",
    "    ax.set_ylim(max_val - 0.5, min_val - 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "def display_missclassified(class_to_idx: Dict[str,int], \n",
    "                           targets: List[int], \n",
    "                           predictions: List[int], \n",
    "                           images: List[np.ndarray], \n",
    "                           gridsize: Tuple[int] = (4,4)):\n",
    "    \"\"\"Display a grid with missclassified samples from test set.\n",
    "    \n",
    "    Args:\n",
    "        class_to_idx: Class to idx encoder\n",
    "        targets:      List containing all ground truths\n",
    "        predictions:  List containing all predictions\n",
    "        images:       List containing image arrays\n",
    "        gridsize:     Tuple describing the final image grid\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plot_counter = 1\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    idx_to_class = {i:label for i, label in enumerate(class_to_idx)}\n",
    "    for i in range(len(targets)):\n",
    "        if plot_counter > gridsize[0]*gridsize[1]:\n",
    "            break\n",
    "        \n",
    "        image = images[i].transpose(1, 2, 0)\n",
    "        image = ((image * std) + mean) * 255\n",
    "        image = image.astype(\"uint8\")\n",
    "    \n",
    "        image = cv2.resize(image, (128, 128))\n",
    "        image = cv2.putText(image, idx_to_class[predictions[i]], (0,20), 3, 0.4, (0,0,255), 1)\n",
    "        if predictions[i] == targets[i]:\n",
    "            pass\n",
    "        else:\n",
    "            ax = fig.add_subplot(gridsize[0], gridsize[1], plot_counter)\n",
    "            ax.imshow(image)\n",
    "            plot_counter += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.histogram(y=x.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "data = {\n",
    "    \"pixel_mean\": 0.5203580774185134,\n",
    "    \"pixel_std\": 0.24102417452995067,\n",
    "    \"labels\": ['negative', 'typical', 'indeterminate', 'atypical']\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"output_file.yaml\", \"w\") as file:\n",
    "    yaml.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'output_file.yaml') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    fruits_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    print(fruits_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../../Datasets/train_test_classification_full_size/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in data_loader:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc9f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
