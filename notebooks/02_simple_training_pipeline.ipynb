{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6182fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import os, glob\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from cutmix.cutmix import CutMix\n",
    "from cutmix.utils import CutMixCrossEntropyLoss\n",
    "\n",
    "from utils.data import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf4706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is SATA SSD\n",
      " Volume Serial Number is D869-DCC1\n",
      "\n",
      " Directory of D:\\Dataset\\Covid19\\train_test_classification_full_size\n",
      "\n",
      "2021-06-15  23:46    <DIR>          .\n",
      "2021-06-15  23:46    <DIR>          ..\n",
      "2021-06-15  23:47    <DIR>          train\n",
      "2021-06-15  23:48    <DIR>          valid\n",
      "               0 File(s)              0 bytes\n",
      "               4 Dir(s)  659ÿ996ÿ516ÿ352 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"D:/Dataset/Covid19/train_test_classification_full_size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcc16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data.yaml', 'r') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "#    data = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53acd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4 #data[\"num_classes\"]\n",
    "RESUME = False\n",
    "epochs = 10\n",
    "IMG_SIZE = 640\n",
    "BATCH_SIZE = 8\n",
    "WEIGHTS_DIR = \"../weights\"\n",
    "CUTMIX = False\n",
    "\n",
    "Path(WEIGHTS_DIR).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e1c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, Lambda\n",
    "from torchvision.transforms import ColorJitter, RandomAffine, RandomPerspective, RandomRotation, RandomErasing, RandomCrop, Grayscale\n",
    "from torchvision.transforms import RandomChoice, RandomApply\n",
    "\n",
    "def get_train_grayscale_transforms(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        RandomApply([\n",
    "            ColorJitter(brightness=0.3, contrast=0.01, saturation=0.01, hue=0),\n",
    "            RandomAffine(0.1, translate=(0.04,0.04), scale=(0.04,0.04), shear=0.01, resample=2),\n",
    "            RandomCrop(30),\n",
    "            RandomPerspective(0.1)\n",
    "        ]),\n",
    "        Resize([img_size, img_size], interpolation=3),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067])\n",
    "    ])\n",
    "def get_test_grayscale_transforms(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        Resize([img_size, img_size], interpolation=3),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067])\n",
    "    ])\n",
    "\n",
    "def show_confusion_matrix(matrix: List[List], labels: List[str]):\n",
    "    \"\"\"Display a nice confusion matrix given\n",
    "    the confusion matrix in a 2D list + list of labels (decoder)\n",
    "    \n",
    "    Args:\n",
    "        matrix: 2D array containing the values to display (confusion matrix)\n",
    "        labels: Array containing the labels (indexed by corresponding label idx)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    min_val, max_val = 0, len(labels)\n",
    "\n",
    "    for i in range(max_val):\n",
    "        for j in range(max_val):\n",
    "            c = matrix[i][j]\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center')\n",
    "\n",
    "    ax.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_xticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_xticklabels(labels, rotation='vertical', fontsize=16)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_yticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_yticklabels(labels, rotation='horizontal', fontsize=16)\n",
    "                    \n",
    "    #ax.set_xlim(min_val, max_val)\n",
    "    ax.set_ylim(max_val - 0.5, min_val - 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "def display_missclassified(class_to_idx: Dict[str,int], \n",
    "                           targets: List[int], \n",
    "                           predictions: List[int], \n",
    "                           images: List[np.ndarray], \n",
    "                           gridsize: Tuple[int] = (4,4)):\n",
    "    \"\"\"Display a grid with missclassified samples from test set.\n",
    "    \n",
    "    Args:\n",
    "        class_to_idx: Class to idx encoder\n",
    "        targets:      List containing all ground truths\n",
    "        predictions:  List containing all predictions\n",
    "        images:       List containing image arrays\n",
    "        gridsize:     Tuple describing the final image grid\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plot_counter = 1\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    idx_to_class = {i:label for i, label in enumerate(class_to_idx)}\n",
    "    for i in range(len(targets)):\n",
    "        if plot_counter > gridsize[0]*gridsize[1]:\n",
    "            break\n",
    "        \n",
    "        image = images[i].transpose(1, 2, 0)\n",
    "        image = ((image * std) + mean) * 255\n",
    "        image = image.astype(\"uint8\")\n",
    "    \n",
    "        image = cv2.resize(image, (128, 128))\n",
    "        image = cv2.putText(image, idx_to_class[predictions[i]], (0,20), 3, 0.4, (0,0,255), 1)\n",
    "        if predictions[i] == targets[i]:\n",
    "            pass\n",
    "        else:\n",
    "            ax = fig.add_subplot(gridsize[0], gridsize[1], plot_counter)\n",
    "            ax.imshow(image)\n",
    "            plot_counter += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e284819e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Succe\\miniconda3\\envs\\yolov5\\lib\\site-packages\\torchvision\\transforms\\transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_imgs = glob.glob(\"D:/Dataset/Covid19/train_test_classification_full_size/train/*/*.jpg\")\n",
    "valid_imgs = glob.glob(\"D:/Dataset/Covid19/train_test_classification_full_size/valid/*/*.jpg\")\n",
    "\n",
    "\n",
    "train_labels = set([os.path.basename(os.path.dirname(img_path)) for img_path in train_imgs])\n",
    "valid_labels = set([os.path.basename(os.path.dirname(img_path)) for img_path in valid_imgs])\n",
    "class_to_idx = {label: idx for idx, label in enumerate(train_labels)}\n",
    "train_dataset = CustomImageDataset(train_imgs, get_test_grayscale_transforms(IMG_SIZE), train_labels)\n",
    "if CUTMIX:\n",
    "    train_dataset = CutMix(train_dataset, num_class=NUM_CLASSES, beta=1.0, prob=0.5, num_mix=3)    # this is paper's original setting for cifar.\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = CustomImageDataset(valid_imgs, get_test_grayscale_transforms(IMG_SIZE), valid_labels)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc26ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['indeterminate', 'atypical', 'negative', 'typical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Succe\\miniconda3\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "1/10 Train: 1.2207: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [05:13<00:00,  2.37it/s]\n",
      "Test: 1.6771: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.275, F1: 0.269, Recall: 0.366, Precision: 0.312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/10 Train: 1.1887: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [04:48<00:00,  2.57it/s]\n",
      "Test: 1.7601: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.295, F1: 0.374, Recall: 0.59, Precision: 0.328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/10 Train: 1.1372: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [04:57<00:00,  2.49it/s]\n",
      "Test: 1.8227: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.41it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.302, F1: 0.385, Recall: 0.605, Precision: 0.466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/10 Train: 1.0673: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [04:50<00:00,  2.55it/s]\n",
      "Test: 1.5131: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.54it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.387, F1: 0.521, Recall: 0.774, Precision: 0.397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/10 Train: 1.0319: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [04:58<00:00,  2.49it/s]\n",
      "Test: 1.4959: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.52it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.37, F1: 0.335, Recall: 0.493, Precision: 0.267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/10 Train: 1.0155: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [04:55<00:00,  2.51it/s]\n",
      "Test: 1.4843: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:13<00:00,  3.58it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.395, F1: 0.534, Recall: 0.79, Precision: 0.414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/10 Train: 0.9961: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [03:49<00:00,  3.24it/s]\n",
      "Test: 1.5097: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.50it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.387, F1: 0.299, Recall: 0.387, Precision: 0.310\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/10 Train: 0.9828: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [05:03<00:00,  2.44it/s]\n",
      "Test: 1.8725: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:14<00:00,  3.48it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.372, F1: 0.516, Recall: 0.745, Precision: 0.471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/10 Train: 0.9743: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 742/742 [05:02<00:00,  2.45it/s]\n",
      "Test: 1.6348: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:13<00:00,  3.76it/s]\n",
      "  0%|                                                                                                                                                                                                                                                                                               | 0/742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Acc: 0.37, F1: 0.501, Recall: 0.74, Precision: 0.401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/10 Train: 0.9687:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                 | 505/742 [02:28<01:06,  3.54it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from utils.models import get_resnet18, get_efficientnetb0\n",
    "\n",
    "# Using gpu or not\n",
    "CUDA = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if CUDA == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"Using CPU\")\n",
    "#model = get_efficientnetb0(NUM_CLASSES, num_channels=1)\n",
    "model = get_resnet18(NUM_CLASSES, num_channels=1)\n",
    "#model = get_ghostnet(NUM_CLASSES)\n",
    "model.to(CUDA)\n",
    "print(list(class_to_idx.keys()))\n",
    "\n",
    "if RESUME:\n",
    "    optimizer, criterion = get_training_stuff(model)\n",
    "else:\n",
    "    #weights = torch.Tensor(weights).to(CUDA)\n",
    "    #optimizer, criterion = get_training_stuff(model, weights=weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    if CUTMIX:\n",
    "        criterion = CutMixCrossEntropyLoss(True)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "if RESUME:\n",
    "    start_epoch = state_dict[\"epoch\"]\n",
    "    optimizer_state_dict = state_dict[\"optimizer_state_dict\"]\n",
    "    best_test_f1 = state_dict[\"best_test_f1\"]\n",
    "\n",
    "    model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state_dict[\"optimizer_state_dict\"])\n",
    "else:\n",
    "    best_test_f1 = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    t = tqdm(train_dataloader)\n",
    "    for i, (X, y) in enumerate(t):\n",
    "        \n",
    "\n",
    "        X = X.to(CUDA)\n",
    "        y = y.to(CUDA)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.cpu().detach()\n",
    "        t.set_description(f\"{epoch+1}/{epochs} Train: {round(float(running_loss)/(i+1), 4)}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = list() # For display purpose\n",
    "        targets = list() # For display purpose\n",
    "        if epoch+1 == epochs:\n",
    "            images = list() # For display purpose\n",
    "        running_loss = 0\n",
    "        t = tqdm(valid_dataloader)\n",
    "        for i, (X, y) in enumerate(t):\n",
    "            X = X.to(CUDA)\n",
    "            y = y.to(CUDA)\n",
    "\n",
    "            preds = model(X)\n",
    "            predictions += list(preds.argmax(axis=1).cpu().detach().numpy())\n",
    "            targets += list(np.array(y.cpu()))\n",
    "            if epoch+1 == epochs:\n",
    "                images += list(np.array(X.cpu()))\n",
    "        \n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            running_loss += loss.cpu().detach()\n",
    "            t.set_description(f\"Test: {round(float(running_loss/(i+1)), 4)}\")\n",
    "\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    f1 = f1_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "    recall = recall_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "    precision = precision_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "    print(f\"Test: Acc: {str(acc)[:5]}, F1: {str(f1)[:5]}, Recall: {str(recall)[:5]}, Precision: {str(precision)[:5]}\\n\")\n",
    "\n",
    "    if f1 > best_test_f1:\n",
    "        best_test_f1 = f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_test_f1': best_test_f1,\n",
    "            'train_dataloader': train_dataloader,\n",
    "            'test_dataloader': valid_dataloader,\n",
    "            'class_to_idx': class_to_idx\n",
    "        }, os.path.join(WEIGHTS_DIR, \"best.pt\"))\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_test_f1': best_test_f1,\n",
    "    'train_dataloader': train_dataloader,\n",
    "    'test_dataloader': valid_dataloader,\n",
    "    'class_to_idx': class_to_idx,\n",
    "}, os.path.join(WEIGHTS_DIR, \"last.pt\"))\n",
    "\n",
    "display_missclassified(class_to_idx, targets, predictions, images, gridsize=(4,4))\n",
    "show_confusion_matrix(confusion_matrix(targets, predictions), list(class_to_idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def show_confusion_matrix(matrix: List[List], labels: List[str]):\n",
    "    \"\"\"Display a nice confusion matrix given\n",
    "    the confusion matrix in a 2D list + list of labels (decoder)\n",
    "    \n",
    "    Args:\n",
    "        matrix: 2D array containing the values to display (confusion matrix)\n",
    "        labels: Array containing the labels (indexed by corresponding label idx)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(20)\n",
    "    fig.set_figwidth(20)\n",
    "\n",
    "    min_val, max_val = 0, len(labels)\n",
    "\n",
    "    for i in range(max_val):\n",
    "        for j in range(max_val):\n",
    "            c = matrix[i][j]\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center')\n",
    "\n",
    "    ax.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_xticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_xticklabels(labels, rotation='vertical', fontsize=16)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_yticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_yticklabels(labels, rotation='horizontal', fontsize=16)\n",
    "                    \n",
    "    #ax.set_xlim(min_val, max_val)\n",
    "    ax.set_ylim(max_val - 0.5, min_val - 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "def display_missclassified(class_to_idx: Dict[str,int], \n",
    "                           targets: List[int], \n",
    "                           predictions: List[int], \n",
    "                           images: List[np.ndarray], \n",
    "                           gridsize: Tuple[int] = (4,4)):\n",
    "    \"\"\"Display a grid with missclassified samples from test set.\n",
    "    \n",
    "    Args:\n",
    "        class_to_idx: Class to idx encoder\n",
    "        targets:      List containing all ground truths\n",
    "        predictions:  List containing all predictions\n",
    "        images:       List containing image arrays\n",
    "        gridsize:     Tuple describing the final image grid\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plot_counter = 1\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    idx_to_class = {i:label for i, label in enumerate(class_to_idx)}\n",
    "    for i in range(len(targets)):\n",
    "        if plot_counter > gridsize[0]*gridsize[1]:\n",
    "            break\n",
    "        \n",
    "        image = images[i].transpose(1, 2, 0)\n",
    "        image = ((image * std) + mean) * 255\n",
    "        image = image.astype(\"uint8\")\n",
    "    \n",
    "        image = cv2.resize(image, (128, 128))\n",
    "        image = cv2.putText(image, idx_to_class[predictions[i]], (0,20), 3, 0.4, (0,0,255), 1)\n",
    "        if predictions[i] == targets[i]:\n",
    "            pass\n",
    "        else:\n",
    "            ax = fig.add_subplot(gridsize[0], gridsize[1], plot_counter)\n",
    "            ax.imshow(image)\n",
    "            plot_counter += 1\n",
    "    plt.show()\n",
    "    \n",
    "display_missclassified(class_to_idx, targets, predictions, images, gridsize=(4,4))\n",
    "show_confusion_matrix(confusion_matrix(targets, predictions), list(class_to_idx.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_efficientnetb0(NUM_CLASSES, num_channels=1)\n",
    "state_dict = torch.load(os.path.join(\"../weights\", \"best.pt\"))\n",
    "model.load_state_dict(state_dict[\"model_state_dict\"])     \n",
    "model.to(CUDA)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = list() # For display purpose\n",
    "    targets = list() # For display purpose\n",
    "    if epoch+1 == epochs:\n",
    "        images = list() # For display purpose\n",
    "    running_loss = 0\n",
    "    t = tqdm(valid_dataloader)\n",
    "    for i, (X, y) in enumerate(t):\n",
    "        X = X.to(CUDA)\n",
    "        y = y.to(CUDA)\n",
    "\n",
    "        preds = model(X)\n",
    "        predictions += list(preds.argmax(axis=1).cpu().detach().numpy())\n",
    "        targets += list(np.array(y.cpu()))\n",
    "        if epoch+1 == epochs:\n",
    "            images += list(np.array(X.cpu()))\n",
    "\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        running_loss += loss.cpu().detach()\n",
    "        t.set_description(f\"Test: {round(float(running_loss/(i+1)), 4)}\")\n",
    "\n",
    "acc = accuracy_score(targets, predictions)\n",
    "f1 = f1_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "recall = recall_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "precision = precision_score(targets, predictions, average=\"macro\", labels=np.unique(predictions))\n",
    "print(f\"Test: Acc: {str(acc)[:5]}, F1: {str(f1)[:5]}, Recall: {str(recall)[:5]}, Precision: {str(precision)[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missclassified(class_to_idx, targets, predictions, images, gridsize=(4,4))\n",
    "show_confusion_matrix(confusion_matrix(targets, predictions), list(class_to_idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e6eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.histogram(y=x.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "data = {\n",
    "    \"pixel_mean\": 0.5203580774185134,\n",
    "    \"pixel_std\": 0.24102417452995067,\n",
    "    \"labels\": ['negative', 'typical', 'indeterminate', 'atypical']\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"output_file.yaml\", \"w\") as file:\n",
    "    yaml.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'output_file.yaml') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    fruits_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    print(fruits_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../../Datasets/train_test_classification_full_size/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in data_loader:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!dir ..\\..\\..\\Dataset\\Covid19\\classification_full_size\\classification_full_size\\train\n",
    "!dir ..\\..\\..\\Dataset\\CIFAR-10-images\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5504c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
