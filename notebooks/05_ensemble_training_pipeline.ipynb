{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prostate-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import os, glob\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from cutmix.cutmix import CutMix\n",
    "from cutmix.utils import CutMixCrossEntropyLoss\n",
    "\n",
    "from utils.models import get_model\n",
    "from utils.data import CustomImageDataset, CustomImageDatasetV2\n",
    "from utils.log import TextDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "figured-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4 \n",
    "RESUME = False\n",
    "epochs = 40\n",
    "IMG_SIZE = 640\n",
    "BATCH_SIZE = 4\n",
    "ACCUM_STEPS = 16\n",
    "WEIGHTS_DIR = \"../weights\"\n",
    "CUTMIX = True\n",
    "#model_names = [\"resnet50\", \"cspresnet50\", \"efficientnet_b1\", \"dpn68\"]\n",
    "\n",
    "model_names = [\"resnet50\", \"cspresnet50\", \"efficientnet_b1\", \"dpn68\"]\n",
    "#model_names = [\"efficientnet_b1\", \"dpn68\"]\n",
    "\n",
    "\n",
    "TRAIN_DATASET = \"../../../Dataset/Covid19/train_test_classification_quarter_size/train\"\n",
    "VALID_DATASET = \"../../../Dataset/Covid19/train_test_classification_quarter_size/valid\"\n",
    "FILENAME_SUFFIX = \"_lr1e-5\"\n",
    "\n",
    "TRAIN_DATASET += \"/*/*.jpg\"\n",
    "VALID_DATASET += \"/*/*.jpg\"\n",
    "Path(WEIGHTS_DIR).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basic-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, Lambda\n",
    "from torchvision.transforms import ColorJitter, RandomAffine, RandomPerspective, RandomRotation, RandomErasing, RandomCrop, Grayscale\n",
    "from torchvision.transforms import RandomChoice, RandomApply\n",
    "\n",
    "from albumentations.core.composition import Compose as ComposeV2\n",
    "import albumentations.augmentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "def get_train_grayscale_transforms_V2(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return ComposeV2([\n",
    "        A.geometric.resize.LongestMaxSize(img_size),\n",
    "        A.geometric.rotate.Rotate(limit=30, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
    "        A.crops.transforms.RandomResizedCrop(img_size, img_size, scale=(0.7, 1.0), ratio=(0.95, 1.05)),\n",
    "        A.transforms.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        A.transforms.Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "def get_test_grayscale_transforms(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return Compose([\n",
    "        Resize([img_size, img_size], interpolation=3),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067])\n",
    "    ])\n",
    "\n",
    "def get_test_grayscale_transforms_V2(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return ComposeV2([\n",
    "        A.geometric.resize.LongestMaxSize(img_size),\n",
    "        A.transforms.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        A.transforms.Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def show_confusion_matrix(matrix: List[List], labels: List[str]):\n",
    "    \"\"\"Display a nice confusion matrix given\n",
    "    the confusion matrix in a 2D list + list of labels (decoder)\n",
    "    \n",
    "    Args:\n",
    "        matrix: 2D array containing the values to display (confusion matrix)\n",
    "        labels: Array containing the labels (indexed by corresponding label idx)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    min_val, max_val = 0, len(labels)\n",
    "\n",
    "    for i in range(max_val):\n",
    "        for j in range(max_val):\n",
    "            c = matrix[i][j]\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center')\n",
    "\n",
    "    ax.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    # Set number of ticks for x-axis001\n",
    "    ax.set_xticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_xticklabels(labels, rotation='vertical', fontsize=16)\n",
    "\n",
    "    # Set number of ticks for x-axis\n",
    "    ax.set_yticks(np.arange(max_val))\n",
    "    # Set ticks labels for x-axis\n",
    "    ax.set_yticklabels(labels, rotation='horizontal', fontsize=16)\n",
    "                    \n",
    "    #ax.set_xlim(min_val, max_val)\n",
    "    ax.set_ylim(max_val - 0.5, min_val - 0.5)\n",
    "    plt.show()\n",
    "    \n",
    "def display_missclassified(class_to_idx: Dict[str,int], \n",
    "                           targets: List[int], \n",
    "                           predictions: List[int], \n",
    "                           images: List[np.ndarray], \n",
    "                           gridsize: Tuple[int] = (4,4)):\n",
    "    \"\"\"Display a grid with missclassified samples from test set.\n",
    "    \n",
    "    Args:\n",
    "        class_to_idx: Class to idx encoder\n",
    "        targets:      List containing all ground truths\n",
    "        predictions:  List containing all predictions\n",
    "        images:       List containing image arrays\n",
    "        gridsize:     Tuple describing the final image grid\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plot_counter = 1\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    idx_to_class = {i:label for i, label in enumerate(class_to_idx)}\n",
    "    for i in range(len(targets)):\n",
    "        if plot_counter > gridsize[0]*gridsize[1]:\n",
    "            break\n",
    "        \n",
    "        image = images[i].transpose(1, 2, 0)\n",
    "        image = ((image * std) + mean) * 255\n",
    "        image = image.astype(\"uint8\")\n",
    "    \n",
    "        image = cv2.resize(image, (128, 128))\n",
    "        image = cv2.putText(image, idx_to_class[predictions[i]], (0,20), 3, 0.4, (0,0,255), 1)\n",
    "        if predictions[i] == targets[i]:\n",
    "            pass\n",
    "        else:\n",
    "            ax = fig.add_subplot(gridsize[0], gridsize[1], plot_counter)\n",
    "            ax.imshow(image)\n",
    "            plot_counter += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incoming-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, device, accumulate_steps=1):\n",
    "    model.train()\n",
    "    results = {\n",
    "        \"running_loss\": 0\n",
    "    }\n",
    "    t = tqdm(train_dataloader)\n",
    "    for i, (X, y) in enumerate(t):\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        \n",
    "        results[\"running_loss\"] += loss.cpu().detach()\n",
    "        loss = loss/accumulate_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if ((i+1) % accumulate_steps) == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        t.set_description(f\"{epoch+1}/{epochs} Train: {round(float(results['running_loss'])/(i+1), 4)}\")\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_model(model, valid_dataloader, device, save_images=False):\n",
    "    results = {\n",
    "        \"running_loss\": 0,\n",
    "        \"targets\": list(),\n",
    "        \"predictions\": list()\n",
    "    }    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if save_images:\n",
    "            results[\"images\"] = list() \n",
    "            \n",
    "        t = tqdm(valid_dataloader)\n",
    "        for i, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = model(X)\n",
    "            results[\"predictions\"] += list(preds.argmax(axis=1).cpu().detach().numpy())\n",
    "            results[\"targets\"] += list(np.array(y.cpu()))\n",
    "            if save_images:\n",
    "                results[\"images\"] += list(np.array(X.cpu()))\n",
    "        \n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            results[\"running_loss\"] += loss.cpu().detach()\n",
    "            t.set_description(f\"Test: {round(float(results['running_loss']/(i+1)), 4)}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "def calculate_metrics():\n",
    "    pass\n",
    "\n",
    "class TrainingResults():\n",
    "    \n",
    "    def __init__(self, metrics):\n",
    "        self.best_results = {metric: [1e99, -1e99] for metric in metrics}\n",
    "    \n",
    "    def isHighest(self, metric, value):\n",
    "        if self.best_results[metric][1] < value:\n",
    "            self.best_results[metric][1] = value\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def isLowest(self, metric, value):\n",
    "        if self.best_results[metric][0] > value:\n",
    "            self.best_results[metric][0] = value\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def loadCheckpoint(self, checkpoint):\n",
    "        self.best_results = checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "according-exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'typical': 0, 'negative': 1, 'indeterminate': 2, 'atypical': 3}\n",
      "{'typical': 0, 'negative': 1, 'indeterminate': 2, 'atypical': 3}\n",
      "{'typical': 0, 'negative': 1, 'indeterminate': 2, 'atypical': 3}\n"
     ]
    }
   ],
   "source": [
    "train_imgs = glob.glob(TRAIN_DATASET)\n",
    "valid_imgs = glob.glob(VALID_DATASET)\n",
    "\n",
    "train_labels = set([os.path.basename(os.path.dirname(img_path)) for img_path in train_imgs])\n",
    "valid_labels = set([os.path.basename(os.path.dirname(img_path)) for img_path in valid_imgs])\n",
    "class_to_idx = {label: idx for idx, label in enumerate(train_labels)}\n",
    "\n",
    "train_dataset = CustomImageDatasetV2(train_imgs, get_train_grayscale_transforms_V2(IMG_SIZE), train_labels)\n",
    "if CUTMIX:\n",
    "    train_dataset = CutMix(train_dataset, num_class=NUM_CLASSES, beta=1.0, prob=0.5, num_mix=2)    # this is paper's original setting for cifar.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = CustomImageDatasetV2(valid_imgs, get_test_grayscale_transforms_V2(IMG_SIZE), valid_labels)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suburban-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all model once\n",
    "for model_name in model_names:\n",
    "    model = get_model(model_name, NUM_CLASSES, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['typical', 'negative', 'indeterminate', 'atypical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/40 Train: 1.226: 100%|███████████████████████████████████████████████████████████| 1334/1334 [05:34<00:00,  3.99it/s]\n",
      "Test: 1.179: 100%|███████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.24it/s]\n",
      "2/40 Train: 0.978:   0%|                                                                      | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.38061837415376165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/40 Train: 1.1352: 100%|██████████████████████████████████████████████████████████| 1334/1334 [05:32<00:00,  4.02it/s]\n",
      "Test: 1.0543: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.23it/s]\n",
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.3908648231962815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/40 Train: 1.1015: 100%|██████████████████████████████████████████████████████████| 1334/1334 [05:36<00:00,  3.97it/s]\n",
      "Test: 1.0177: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.26it/s]\n",
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.3856786316697324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/40 Train: 1.0903: 100%|██████████████████████████████████████████████████████████| 1334/1334 [05:35<00:00,  3.98it/s]\n",
      "Test: 0.9991: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.25it/s]\n",
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.3955816243197655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/40 Train: 1.079: 100%|███████████████████████████████████████████████████████████| 1334/1334 [05:33<00:00,  4.00it/s]\n",
      "Test: 0.9775: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.17it/s]\n",
      "6/40 Train: 1.1615:   0%|                                                                     | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.403704368002076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/40 Train: 1.065: 100%|███████████████████████████████████████████████████████████| 1334/1334 [05:33<00:00,  4.00it/s]\n",
      "Test: 0.9708: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.25it/s]\n",
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.400403370452065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/40 Train: 1.0647: 100%|██████████████████████████████████████████████████████████| 1334/1334 [06:00<00:00,  3.70it/s]\n",
      "Test: 0.9765: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:41<00:00,  6.08it/s]\n",
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.4103063631020981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/40 Train: 1.0565: 100%|██████████████████████████████████████████████████████████| 1334/1334 [05:45<00:00,  3.87it/s]\n",
      "Test: 0.9662: 100%|██████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.24it/s]\n",
      "  0%|                                                                                         | 0/1334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final balanced accuracy: 0.4125102082872211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/40 Train: 1.0589: 100%|██████████████████████████████████████████████████████████| 1334/1334 [05:33<00:00,  3.99it/s]\n",
      "Test: 1.2058:  61%|████████████████████████████████████████▏                         | 152/250 [00:20<00:14,  6.84it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "# Using gpu or not\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"Using CPU\")\n",
    "    \n",
    "for model_name in model_names:\n",
    "    model = get_model(model_name, NUM_CLASSES, 1)\n",
    "    model.to(device)\n",
    "    print(list(class_to_idx.keys()))\n",
    "\n",
    "    results_document = TextDocument(f\"{model_name}{FILENAME_SUFFIX}_results.txt\")\n",
    "    results_document.add_line(f\"acc balanced_acc f1 recall precision valid_loss train_loss\") \n",
    "    metrics = [\"balanced_acc\"]\n",
    "    training_results = TrainingResults(metrics)\n",
    "    \n",
    "    if RESUME:\n",
    "        pass\n",
    "        #start_epoch = state_dict[\"epoch\"]\n",
    "        #optimizer_state_dict = state_dict[\"optimizer_state_dict\"]\n",
    "        \n",
    "        #model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "        #optimizer.load_state_dict(state_dict[\"optimizer_state_dict\"])\n",
    "        training_results.loadCheckpoint(state_dict[\"training_results\"])\n",
    "    else:\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        if CUTMIX:\n",
    "            criterion = CutMixCrossEntropyLoss(True)\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        \n",
    "        # Train one epoch    \n",
    "        results = train_one_epoch(model, train_dataloader, device, accumulate_steps=ACCUM_STEPS)\n",
    "        train_loss = float(results[\"running_loss\"])\n",
    "\n",
    "\n",
    "        if epoch+1 == epochs:\n",
    "            results = evaluate_model(model, valid_dataloader, device, save_images=True)\n",
    "            images = results[\"images\"]\n",
    "        else:\n",
    "            results = evaluate_model(model, valid_dataloader, device, save_images=False)\n",
    "\n",
    "        valid_loss = float(results[\"running_loss\"])\n",
    "        acc = accuracy_score(results[\"targets\"], results[\"predictions\"])\n",
    "        f1 = f1_score(results[\"targets\"], results[\"predictions\"], average=\"macro\", labels=np.unique(results[\"predictions\"]))\n",
    "        recall = recall_score(results[\"targets\"], results[\"predictions\"], average=\"macro\", labels=np.unique(results[\"predictions\"]))\n",
    "        precision = precision_score(results[\"targets\"], results[\"predictions\"], average=\"macro\", labels=np.unique(results[\"predictions\"]))\n",
    "        balanced_acc = balanced_accuracy_score(results[\"targets\"], results[\"predictions\"])\n",
    "        \n",
    "        \n",
    "        results_document.add_line(f\"{float(acc)} {float(balanced_acc)} {float(f1)} {float(recall)} {float(precision)} {valid_loss} {train_loss}\")\n",
    "        \n",
    "        if training_results.isHighest('balanced_acc',  balanced_acc):\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'training_results': training_results.best_results,\n",
    "                'train_dataloader': train_dataloader,\n",
    "                'test_dataloader': valid_dataloader,\n",
    "                'class_to_idx': class_to_idx\n",
    "            }, os.path.join(WEIGHTS_DIR, f\"{model_name}{FILENAME_SUFFIX}_best_balanced_acc.pt\"))\n",
    "        \"\"\"\n",
    "        if training_results.isHighest('f1',  f1):\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'training_results': training_results.best_results,\n",
    "                'train_dataloader': train_dataloader,\n",
    "                'test_dataloader': valid_dataloader,\n",
    "                'class_to_idx': class_to_idx\n",
    "            }, os.path.join(WEIGHTS_DIR, f\"{model_name}{FILENAME_SUFFIX}_best_f1.pt\"))\n",
    "        if training_results.isHighest('acc', acc):\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'training_results': training_results.best_results,\n",
    "                'train_dataloader': train_dataloader,\n",
    "                'test_dataloader': valid_dataloader,\n",
    "                'class_to_idx': class_to_idx\n",
    "            }, os.path.join(WEIGHTS_DIR, f\"{model_name}{FILENAME_SUFFIX}_best_acc.pt\"))\n",
    "        if training_results.isLowest('valid_loss', valid_loss):\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'training_results': training_results.best_results,\n",
    "                'train_dataloader': train_dataloader,\n",
    "                'test_dataloader': valid_dataloader,\n",
    "                'class_to_idx': class_to_idx\n",
    "            }, os.path.join(WEIGHTS_DIR, f\"{model_name}{FILENAME_SUFFIX}_best_valid_loss.pt\"))\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'training_results': training_results.best_results,\n",
    "            'train_dataloader': train_dataloader,\n",
    "            'test_dataloader': valid_dataloader,\n",
    "            'class_to_idx': class_to_idx\n",
    "        }, os.path.join(WEIGHTS_DIR, f\"{model_name}{FILENAME_SUFFIX}_last.pt\"))\n",
    "        print(f\"Final balanced accuracy: {balanced_acc}\")\n",
    "        \n",
    "    #display_missclassified(class_to_idx, targets, predictions, images, gridsize=(4,4))\n",
    "    #show_confusion_matrix(confusion_matrix(targets, predictions), list(class_to_idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_missclassified(class_to_idx, targets, predictions, images, gridsize=(4,4))\n",
    "show_confusion_matrix(confusion_matrix(targets, predictions), list(class_to_idx.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        data = f.readlines()\n",
    "    columns = data[0].replace('\\n', '').split()\n",
    "    rows = [line.replace('\\n', '').split() for line in data[1:]]\n",
    "    for idx, row in enumerate(rows):\n",
    "        rows[idx] = [float(i) for i in row]\n",
    "    results_dict = {column: [row[i] for row in rows] for i, column in enumerate(columns)}\n",
    "    return results_dict\n",
    "\n",
    "model_names = [\"resnet50\", \"cspresnet50\", \"efficientnet_b1\", \"dpn68\"]\n",
    "fig, axs = plt.subplots(1,3, figsize=(30,10))\n",
    "for model_name in model_names:\n",
    "    txt_path = model_name + \"_results.txt\"\n",
    "    data = read_results(txt_path)\n",
    "\n",
    "    axs[0].plot(data['f1'])\n",
    "    axs[1].plot(data['acc'])\n",
    "    axs[2].plot(data['valid_loss'])\n",
    "axs[0].set_title(\"F1\")\n",
    "axs[1].set_title(\"Accuracy\")\n",
    "axs[2].set_title(\"valid_loss\")\n",
    "axs[0].legend(model_names)\n",
    "axs[1].legend(model_names)\n",
    "axs[2].legend(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9178e-886d-4161-a4d5-f90e7e27ab1d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d3423-d140-4af9-af38-e39accb97c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "import albumentations.augmentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.core.composition import Compose as ComposeV2\n",
    "\n",
    "from utils.models import get_model\n",
    "from utils.data import InferenceImageDatasetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136f929-1a7b-4d72-913a-482c6ce1f208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VALID_DATASET = \"../../../Dataset/Covid19/train_test_classification_quarter_size/valid\"\n",
    "model_names = [\"resnet50\", \"cspresnet50\", \"efficientnet_b1\", \"dpn68\"]\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "#{'negative': 0, 'indeterminate': 1, 'atypical': 2, 'typical': 3}\n",
    "label_names = ['indeterminate', 'negative', 'atypical', 'typical']\n",
    "IMG_SIZE = 640\n",
    "\n",
    "pt_paths = glob.glob(\"../weights/**/*.pt\", recursive=True) \n",
    "model_pt_dict = dict()\n",
    "for pt_path in pt_paths:\n",
    "    for model_name in model_names:\n",
    "        if model_name in pt_path:\n",
    "            #print(model_name, pt_path)\n",
    "            model_pt_dict[pt_path] = model_name\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a4fb5-0300-4ee8-a859-74dd6832b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_test_grayscale_transforms_V2(img_size: int) -> Compose:\n",
    "    \"\"\"Returns data transformations/augmentations for train dataset.\n",
    "    \n",
    "    Args:\n",
    "        img_size: The resolution of the input image (img_size x img_size)\n",
    "    \"\"\"\n",
    "    return ComposeV2([\n",
    "        A.geometric.resize.LongestMaxSize(img_size),\n",
    "        A.transforms.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        A.transforms.Normalize(\n",
    "            mean=[0.5203580774185134],\n",
    "            std=[0.24102417452995067]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def inference(model, img_paths, device=\"cpu\"):\n",
    "    \n",
    "    dataset = InferenceImageDatasetV2(img_paths, get_test_grayscale_transforms_V2(IMG_SIZE), label_names)\n",
    "    dataloader = DataLoader(dataset, batch_size=32)\n",
    "    print(dataloader)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = list() \n",
    "    image_paths = list()\n",
    "    with torch.no_grad():\n",
    "        for X, img_paths in tqdm(dataloader):\n",
    "\n",
    "            X = X.to(device)\n",
    "            preds = model(X)\n",
    "            predictions += list(preds.cpu().detach().numpy())\n",
    "            image_paths += list(img_paths)\n",
    "    model.cpu()\n",
    "\n",
    "    return predictions, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gpu or not\n",
    "CUDA = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if CUDA == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "ensemble_models = list()\n",
    "ensemble_predictions = list()\n",
    "ensemble_img_paths = list()\n",
    "valid_imgs = glob.glob(VALID_DATASET + \"/*/*.jpg\")\n",
    "for pt_path in pt_paths:\n",
    "    if model_pt_dict.get(pt_path) is not None:\n",
    "        model_name = model_pt_dict[pt_path]\n",
    "        model = get_model(model_name, input_channels=1, num_classes=NUM_CLASSES)\n",
    "        state_dict = torch.load(pt_path)\n",
    "        model.load_state_dict(state_dict[\"model_state_dict\"])  \n",
    "        print(f\"'{pt_path}' loaded into '{model_name}'\")\n",
    "        #print(valid_imgs)\n",
    "        predictions, image_paths = inference(model, valid_imgs, device=\"cuda\")\n",
    "        ensemble_img_paths.append(image_paths)\n",
    "        ensemble_predictions.append(predictions)\n",
    "        ensemble_models.append((model_name, pt_path))\n",
    "    else:\n",
    "        print(f\"'{pt_path}' cannot be linked to model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461cce3-bc5d-4a7e-aba0-f0602f59e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\"\"\"\n",
    "ensemble_img_paths # List of paths\n",
    "ensemble_predictions # List of predictions\n",
    "ensemble_models #list of model and pt names \n",
    "\"\"\"\n",
    "results = dict()\n",
    "for img_path in set([path for paths in ensemble_img_paths for path in paths]):\n",
    "    results[img_path] = list()\n",
    "\n",
    "    \n",
    "for img_paths, predictions, (model_name, pt_path) in tqdm(zip(ensemble_img_paths, ensemble_predictions, ensemble_models)):\n",
    "    for img_path, prediction in zip(img_paths, predictions):\n",
    "        filename = f\"{os.path.basename(img_path)}#{model_name}#{os.path.basename(pt_path)}\"\n",
    "        save_path = f\"../results/predictions/{os.path.basename(img_path)}\"\n",
    "        Path(save_path).mkdir(exist_ok=True, parents=True)\n",
    "        torch.save(prediction, os.path.join(save_path, filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92e155-3082-4d68-9df2-a08989f3aa33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ensemble_img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb8950-907e-4fcd-bc31-80146811f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5002ea-b39c-4b19-bd59-ea04a65d1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6626d3-a6c3-4147-90c5-9ee5cab029e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_names = ['indeterminate', 'negative', 'atypical', 'typical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78e301-f19e-4f05-9eb7-65b00da2b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img_path = list()\n",
    "temp_0 = list()\n",
    "temp_1 = list()  \n",
    "temp_2 = list()\n",
    "temp_3 = list()\n",
    "temp_model_name = list()\n",
    "temp_model_pt_path = list()\n",
    "\n",
    "for img_paths, predictions, (model_name, pt_path) in tqdm(zip(ensemble_img_paths, ensemble_predictions, ensemble_models)):\n",
    "    for img_path, prediction in zip(img_paths, predictions):\n",
    "        temp_img_path.append(img_path)\n",
    "        temp_0.append(prediction[0])\n",
    "        temp_1.append(prediction[1])    \n",
    "        temp_2.append(prediction[2])\n",
    "        temp_3.append(prediction[3])\n",
    "        temp_model_name.append(model_name)\n",
    "        temp_model_pt_path.append(pt_path)\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame({\n",
    "    \"image_path\":       temp_img_path,\n",
    "    \"00_indeterminate\": temp_0,\n",
    "    \"01_negative\":      temp_1,\n",
    "    \"02_atypical\":      temp_2,\n",
    "    \"03_typical\":       temp_3,\n",
    "    \"model_name\":       temp_model_name,\n",
    "    \"model_pt_path\":    temp_model_pt_path,\n",
    "})\n",
    "df.to_csv(\"model_logits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55168d-e5a9-4456-b787-1c4c0bed00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c20c5-a064-4179-8e42-4c2b8c7ae70a",
   "metadata": {},
   "source": [
    "# Ensemble calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558acf3d-72e0-408f-8adb-f8999a1c1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76a0be-e7fb-4b3b-8055-ef4d6c73d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"model_logits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cba92-d6b9-498d-a1b3-096a6c2b09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_cols = [\"00_indeterminate\", \"01_negative\", \"02_atypical\", \"03_typical\"]  \n",
    "#logit_cols = [\"0_prob\", \"1_prob\", \"2_prob\", \"3_prob\"]\n",
    "temp = df[logit_cols].apply(softmax, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d26ba-50b0-49b6-b32b-2e3f1ac0b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in logit_cols:\n",
    "    temp.rename(columns={\n",
    "        col: col+\"_prob\",\n",
    "    }, inplace=True)\n",
    "df = df.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5dd35-2e4d-4dc5-ad35-91d3ee46d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df[\"image_path\"].str.split(\"\\\\\").apply(lambda x: x[-2])\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dc209-0561-49dd-a3c8-10fcbbe9f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"indeterminate\", \"negative\", \"atypical\", \"typical\"]  \n",
    "class2idx = {col:idx for idx, col in enumerate(class_names)}\n",
    "ground_truth_indices = ground_truth.apply(lambda x: class2idx[x])\n",
    "ground_truth_indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992324f2-6219-40ac-a60e-90f96b25c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ground_truth\"] = ground_truth\n",
    "df[\"ground_truth_indices\"] = ground_truth_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a2030-6520-45e4-bf6f-21940744a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prediction\"] = df.apply(lambda x: x[[\"00_indeterminate_prob\", \"01_negative_prob\", \"02_atypical_prob\", \"03_typical_prob\"]].to_numpy().argmax(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90c8ad-6e7e-42fa-84a4-967fec0726bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df[\"model_pt_path\"].unique()\n",
    "image = df[\"image_path\"].unique()\n",
    "\n",
    "def get_sample(image_name, model_name):\n",
    "    row = df[df[\"image_path\"] == image_name]\n",
    "    X = row[row[\"model_pt_path\"] == model_name][[\"00_indeterminate_prob\", \"01_negative_prob\", \"02_atypical_prob\", \"03_typical_prob\"]]\n",
    "    y = row[row[\"model_pt_path\"] == model_name][\"ground_truth\"]\n",
    "    #print(row[row[\"model_pt_path\"] == model_name][\"prediction\"])\n",
    "    return X, y\n",
    "get_sample(image[0], models[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb39eb4-6aeb-45aa-b679-1b716f620705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score , classification_report, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa404e-03f7-4e36-af30-ed7b427f60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_pt_path in models:\n",
    "    temp_df = df[df[\"model_pt_path\"] == model_pt_path]\n",
    "    y_pred = temp_df[\"prediction\"]\n",
    "    y_true = temp_df[\"ground_truth_indices\"]\n",
    "    \n",
    "    acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    print(acc, model_pt_path)\n",
    "    #print(classification_report(y_true, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96b1ed-dac8-4ada-85b6-51a682020ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.rand([num_models])\n",
    "\n",
    "final_prediction = None\n",
    "for weight, prediction in zip(weights, predictions):\n",
    "    if final_prediction is None:\n",
    "        final_prediction = weight * prediction\n",
    "    else:\n",
    "        final_prediction += weight * prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
